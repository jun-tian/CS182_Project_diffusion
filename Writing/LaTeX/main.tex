\documentclass[a4paper]{article} 
\input{head}
\begin{document}

%-------------------------------
%	TITLE SECTION
%-------------------------------
\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth} 
\raggedright
\footnotesize
Group Name: \hfill \\
Berkeley Goggles
\end{minipage}
\begin{minipage}{0.4\textwidth} 
\centering 
\large 
Project Question Design\\ 
\normalsize 
CS182\\ 
\end{minipage}
\begin{minipage}{0.295\textwidth} 
\raggedleft
\today\hfill\\
\end{minipage}
\medskip\hrule 
\bigskip
%-------------------------------
%	Problem
%-------------------------------
\section{Paper Introduction}
This homework explores Latent Diffusion Models as presented in \href{https://arxiv.org/pdf/2112.10752.pdf}{this paper}. Stable diffusion is a text conditional latent diffusion model, created by researchers and engineers from CompVis, Stability AI, and LAION. Diffusion models are incredibly expensive to the train so with the use of latent space, we can greatly reduce the computational resource requirements for training, and also make comparable 

\subsection{Autoencoders and Downsampling}
Autoencoders downsample the input images by a factor of \(f=2^m, m  \, \epsilon \, \mathbb{R}\). The paper experiments with 6 different sampling factors \(f={1, 2, 4, 8, 16, 32}\) abbrieviated as LDM-f. LDM-1 represents pixel-based diffusion models with no downsampling. The paper tested the performance on these 6 models and qualified performance with two different scores: \href{https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance}{FID (Frechet Inception Distance)} which compares the distribution of images generated with the model with distribution of real images and \href{https://en.wikipedia.org/wiki/Inception_score}{IS (Inception Score)} which measures the quality of images generated by the model.
\begin{itemize}
    \item [(a)]
    Given two graphs that show the FID scores and IS scores against training progress, we can see that three of the LDM-f models perform well and three have weaker performance. Identify the top three performing LDM-f models and the bottom three performing LDM-f models. Provide a brief explanation for your answer.

     \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.4\textwidth]{fid_scores.png} \hfill
        \includegraphics[width=0.4\textwidth]{is_scores.png}
        \caption{IS Scores vs Training Progress}
        \label{fig:is}
    \end{figure}

     \textcolor{blue}{Solution: The top three performing models are LDM-4, LDM-8, LDM-16. The worst of the three models are LDM-1, LDM-2, and LDM-32. With LDM-1 and LDM-2, the performance is too slow and we see the same issues that occur with the large training time of diffusion models. LDM-32 results in information loss due to too much compression. With a downsampling factor \(f = [4, 16]\) we can reap the benefits of faster performance without having losing too much information about the image in the process.}
     
     
\end{itemize}

\subsection{Conditional LDM Loss}
We can think of diffusion models as a sequence of denoising autoencoders \(\epsilon_{\theta}(x_t, t)\); \(t = 1...T\) where \(x_t\) is a noisy version of the input and  \(t\) is sampled uniformly. We get the following loss:

\[L_{LDM} = \mathbb{E}_{\varepsilon(x), \varepsilon ~ \mathcal{N}(0, 1), t} [\| \varepsilon - \varepsilon_{\theta}(x_t, t) \|_2^2 ] \]

For a conditional LDM, we use a conditional denoising autoencoder which the latent representation, \(z_t\) and a domain specific encoder \(\tau_{\theta}\) that projects a text input \(y\) into an intermediate representation. 
\begin{itemize}
    \item [(b)]
    Write the loss function for a conditional LDM.
   
\end{itemize}
 \textcolor{blue}{Solution: \[L_{LDM} = \mathbb{E}_{\varepsilon(x), y, \varepsilon ~ \mathcal{N}(0, 1)} [\| \varepsilon - \varepsilon_{\theta}(z_t, t, \tau_{\theta}(y)) \|_2^2] \]}


%-------------------------------
%	Problem
%-------------------------------
\section{Image Synthesis with Diffusion Model}

Please follow the instructions in this \href{https://colab.research.google.com/drive/1mmyvwlYvAnnfIBIr29bMvOEnL2mUcPHm?usp=sharing}{notebook}. 
You will implement a text conditioned denoising unet and the sampling algorithm. Then you'll see the diffusion process on a single image and try to train your own diffusion mode. Once you finished with the notebook,
\begin{itemize}
    \item download submission.zip and submit it to Gradescope.

    \item Answer the following questions in your submission of the written assignment:
\end{itemize}

\begin{itemize}
    \item [(a)]
    Why might a U-Net by a good choice for the model backbone? List two reasons.

    \textcolor{blue}{Solution:
    (This is an open question, if the explanation of the answer is reasonable, then it can be considered correct)
    \begin{itemize}
        \item The model must output the amount of noise in the input, which is of the same shape as the input, meaning both the input and output are of identical shape. This is exactly what a U-Net does.
        \item UNet has an inductive bias geared towards image-like data. This is due to using convolutions and the downsampling followed by upsampling with skip connections between them which gives it the ability to pick out hierarchical and spatial structures in data.
    \end{itemize}
}

    \item [(b)]
    Screenshot your visualization for get noisy image and include it in your submission of the written assignment. And describe the picture you observed briefly. What kind of process is this?

    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{sample.png}
        \caption{Visualization for Sampling}
        \label{fig:sample}
    \end{figure}
    \textcolor{blue}{Solution: Shown as Abbildung \ref{fig:sample}. This should be a process of gradually adding noise to the original picture. Corresponding to the q sample in diffusion model algorithm.
    }

    \item [(c)]
    Screenshot one of your visualizations about the sampled images and include it in your submission of the written assignment. Then answer the following question. How does the model perform and does it meet your expectations? If not, what do you think are the directions for improvement?

    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.3\textwidth]{performance.png}
        \caption{Visualization for Model Performance}
        \label{fig:perf}
    \end{figure}

    \textcolor{blue}{Solution: The performance of the model was not so great and did not meet expectations. As can be seen from the visualization picture, it only learns the style of the dataset, but does not learn the real semantic information. The main reason is that the datasets and models we use are too small. At the same time, MNIST is not a standard image-text pair dataset, so our text definition is not very accurate. In addition, clip does not perform particularly well with handwritten digits. In order to improve the performance of the model, we also need to consider using a larger and more professional image-text pair dataset while increasing the scale of the model. This further underscores the current trend towards larger models.}
\end{itemize}

\end{document}
