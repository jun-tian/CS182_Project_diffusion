{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eEHc9SNKLS7l"
      },
      "source": [
        "# Implement Diffusion Models from Scratch\n",
        "In this assignment, you will implement a small version of the famous [stable diffusion](https://arxiv.org/abs/2112.10752).\n",
        "\n",
        "You will \n",
        "- Implement each module in the diffusion model to increase your understanding of its composition.\n",
        "- Implement a simple noise sampling process to understand the process of diffusion\n",
        "- Use the model you constructed to train on the mnist dataset, observe the sampling results, and experience the power of the diffusion model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nvT2bGN2LS7m"
      },
      "source": [
        "## Setup\n",
        "We recommend working on Colab with GPU enabled since this assignment needs a fair amount of compute.\n",
        "In Colab, we can enforce using GPU by clicking `Runtime -> Change Runtime Type -> Hardware accelerator` and selecting `GPU`.\n",
        "The dependencies will be installed once the notebook cells are excuted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1huIFYblrioO"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install einops\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "T1J7HtH-LS7n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import math\n",
        "import urllib\n",
        "import random\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "\n",
        "from datasets import load_dataset\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import datetime\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP2rnJZLrgaf"
      },
      "outputs": [],
      "source": [
        "#@title random seed\n",
        "def set_seed(seed: int = 3207, verbose=False) -> None:\n",
        "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    if verbose:\n",
        "        print(f\"Random seed set as {seed}\")\n",
        "\n",
        "set_seed(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "k7h9THz-YIM3"
      },
      "outputs": [],
      "source": [
        "#@title Utilities for Testing\n",
        "def save_auto_grader_data():\n",
        "    torch.save(\n",
        "        {'output': auto_grader_data['output']},\n",
        "        'autograder.pt'\n",
        "    )\n",
        "\n",
        "def rel_error(x, y):\n",
        "    return torch.max(\n",
        "        torch.abs(x - y)\n",
        "        / (torch.maximum(torch.tensor(1e-8), torch.abs(x) + torch.abs(y)))\n",
        "    ).item()\n",
        "\n",
        "def check_error(name, x, y, tol=1e-3):\n",
        "    error = rel_error(x, y)\n",
        "    if error > tol:\n",
        "        print(f'The relative error for {name} is {error}, should be smaller than {tol}')\n",
        "    else:\n",
        "        print(f'The relative error for {name} is {error}')\n",
        "\n",
        "def check_loss(loss, threshold):\n",
        "    if loss > threshold:\n",
        "        print(f'The minimum loss {loss} should <= threshold loss {threshold}')\n",
        "    else:\n",
        "        print(f'The minimum loss {loss} is smaller than threshold loss {threshold}')\n",
        "\n",
        "def load_from_url(url):\n",
        "    return torch.load(io.BytesIO(urllib.request.urlopen(url).read()))\n",
        "\n",
        "test_data = load_from_url('https://github.com/jun-tian/CS182_Project_diffusion/raw/main/Diffusion/test_data.pt')\n",
        "auto_grader_data = load_from_url('https://github.com/jun-tian/CS182_Project_diffusion/raw/main/Diffusion/auto_grader_data.pt')\n",
        "auto_grader_data['output'] = {}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8wWQqcv6oFQw"
      },
      "source": [
        "## Implement Modules in the Diffusion Model\n",
        "\n",
        "Below, you'll implement different modules in the diffusion model, including Resnet blocks, Spatial Transformer blocks and the U-Net backbone. It's important to note that this implementation is a simplified version of the official stable diffusion architecture used in real applications. To make it easier to implement, we'll offer some architecture diagrams to help you understand those modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XlnjDNIxLS7o"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions \n",
        "# (You will utilize some of the helper functions in your implementation)\n",
        "\n",
        "def exists(x):\n",
        "    \"\"\"return true if x is not none\"\"\"\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    \"\"\"return val if exists(val) else d\"\"\"\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "# normalization functions\n",
        "def normalize_to_neg_one_to_one(img):\n",
        "    return img * 2 - 1\n",
        "\n",
        "def unnormalize_to_zero_to_one(t):\n",
        "    return (t + 1) * 0.5\n",
        "\n",
        "def normalization(dim):\n",
        "    return nn.GroupNorm(num_groups=16, num_channels=dim)\n",
        "\n",
        "# upsample and downsample\n",
        "def Upsample(dim, dim_out = None):\n",
        "    return nn.Sequential(\n",
        "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
        "        nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1)\n",
        "    )\n",
        "\n",
        "def Downsample(dim, dim_out = None):\n",
        "    return nn.Sequential(\n",
        "        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2),\n",
        "        nn.Conv2d(dim * 4, default(dim_out, dim), 1)\n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W2h_qJicPypB"
      },
      "source": [
        "### ResNet Module\n",
        "First, you'll need to implement the ResNet Module below. Note that this Resnet is different than what you have learned in lecture before, because we need to combine the timestep into the forward pass. For here, you don't need know what timestep is yet, just remember it is a tensor of size `[Batchsize, time_emb_dim]`.\n",
        "To help you get the correct answer, we put a architecture diagram in the below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMG-4gzBv2ni"
      },
      "source": [
        "<img src=\"https://github.com/jun-tian/CS182_Project_diffusion/blob/main/images/resnet.png?raw=true\" alt=\"resnt\" width=\"600\" height=\"500\" align=\"bottom\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GDWsuqf9LS7p"
      },
      "outputs": [],
      "source": [
        "#@title Build Your Resnet\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, time_emb_dim = None, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          dim: input dimension\n",
        "          dim_out: output dimension\n",
        "          time_emb_dim: the dimension for the timestep embedding\n",
        "          drop_out: probability of dropout\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        ########################################################################\n",
        "        # TODO: Define  layers for time_embed, input, output and skip connection.\n",
        "        #       Hint: 1. Use nn.Sequential to contain different layers in one element\n",
        "        #             2. We use SiLU as the out activation function\n",
        "        #             3. Feel free to use the helper function normalization defined above\n",
        "        #             4. The skip connection may require a Conv if the input and output\n",
        "        #                dimensions do not match\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input tensor [B, C, H, W]\n",
        "            timestep: timestep [B, time_emb_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        time_emb = None\n",
        "        scale_shift = None\n",
        "        h = None\n",
        "        output = None\n",
        "\n",
        "        ########################################################################\n",
        "        # TODO: Implement the forward pass of the `ResnetBlock` class.\n",
        "        #       Hint: 1. Please refer to the diagram above\n",
        "        #             2. einops.rearrange and tensor.chunk() function maybe useful\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_0Tlcwirdce"
      },
      "outputs": [],
      "source": [
        "#@title Test for Resnet\n",
        "dim = 32\n",
        "dim_out = 32\n",
        "time_emb_dim = 10\n",
        "dropout = 0\n",
        "batchsize = 10\n",
        "image_size = 32\n",
        "model = ResnetBlock(dim, dim_out, time_emb_dim, dropout)\n",
        "\n",
        "# test\n",
        "model.load_state_dict(test_data['weights']['resnet'])\n",
        "x, timestep = test_data[\"input\"][\"resnet\"]\n",
        "y = test_data[\"output\"][\"resnet\"]\n",
        "output = model(x, timestep).detach()\n",
        "check_error(\"resnet\", output, y)\n",
        "\n",
        "\n",
        "# auto_grader  \n",
        "model.load_state_dict(auto_grader_data['weights']['resnet'])\n",
        "x, timestep = auto_grader_data[\"input\"][\"resnet\"]\n",
        "output = model(x, timestep).detach()\n",
        "auto_grader_data[\"output\"][\"resnet\"] = output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7HHFh8L-93fT"
      },
      "source": [
        "### Text Embedding\n",
        "A key difference from past diffusion models is that latent diffusion models incorporate general-purpose conditioning mechanisms to condition the diffusion process with, for example, text, images, or layout maps. This opens the way for image generation from a prompt, image-to-image generation, and more.\n",
        "\n",
        "This is achieved by creating an embedding of the conditioning $y$ using a domain specific encoder $\\tau_{\\theta}$. In the case of text conditioning, this can be done by using a CLIP or BERT embedder to first generate token embeddings and then passing the embeddings through a transformer. In this demo we will be using CLIP embeddings along with the frozen CLIP transformer. \n",
        "\n",
        "This conditioning is incorporated into the model by augmenting the U-Net with cross-attention, where the keys and values are created from $\\tau_{\\theta}(y)$ and the queries come from the representation of the U-Net at the specific layer. The same conditioning $\\tau_{\\theta}(y)$ is incorporated in each attention mechanism at every denoising iteration.\n",
        "\n",
        "<img src=\"https://github.com/jun-tian/CS182_Project_diffusion/blob/main/images/ldm.png?raw=true\" width=\"650\"/>\n",
        "\n",
        "Since it is based on the CLIP API, we have implemented it for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iyTpajmh9ym7"
      },
      "outputs": [],
      "source": [
        "class FrozenCLIPEmbedder(nn.Module):\n",
        "    \"\"\"Uses the CLIP transformer encoder for text (from Hugging Face)\"\"\"\n",
        "    def __init__(self, version=\"openai/clip-vit-base-patch16\", device=\"cuda\", max_length=77):\n",
        "        super().__init__()\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
        "        self.transformer = CLIPTextModel.from_pretrained(version)\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "        self.freeze()\n",
        "\n",
        "    def freeze(self):\n",
        "        self.transformer = self.transformer.eval()\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, text):\n",
        "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
        "        outputs = self.transformer(input_ids=tokens)\n",
        "\n",
        "        z = outputs.last_hidden_state\n",
        "        return z\n",
        "\n",
        "    def encode(self, text):\n",
        "        return self(text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n1WU59Hahx0R"
      },
      "source": [
        "### Attention Module\n",
        "\n",
        "Next, we need to implement the attention module in the diffusion model. As we know, stable diffusion is a text conditional model. So here we will use a Spatial Transformer to incorporate textual information into our forward pass process. \n",
        "\n",
        "Below, we divide the Spatial Transformer into four classes. Implementing the first three classes can help you to understand the Spatial Transformer more easily. This is a modular programming thought."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hvXynAPLLS7p"
      },
      "outputs": [],
      "source": [
        "#@title Build Your Spatial Transformer\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Feed Forward layer with GELU activation and dropout.\"\"\"\n",
        "    def __init__(self, dim, dim_out=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = None\n",
        "        ########################################################################\n",
        "        # TODO: Define layers for the `FeedForward` class.\n",
        "        #       Hint: 1. Linear -> GELU -> Dropout -> Linear\n",
        "        #             2. inner dim = mult x dim\n",
        "        #             3. If dim_out is None, then just use dim as the output dimension\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"A standard multi heads Cross Attention layer\"\"\"\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "        self.heads = heads\n",
        "\n",
        "        self.scale = None\n",
        "        self.to_q = None\n",
        "        self.to_k = None\n",
        "        self.to_v = None\n",
        "        self.to_out = None\n",
        "\n",
        "        ########################################################################\n",
        "        # TODO: Define layers for the `CrossAttention` class.\n",
        "        #       Hint: 1. Define the weights for q, k, v. (Linear layer without bias)\n",
        "        #             2. scale is a coefficient for scaled dot attention\n",
        "        #             3. to_out is the output projection (Linear->Dropout)\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: input tensor [B, C, H, W] or [B, (H, W), C]\n",
        "          context: text embedding [B, K, E]\n",
        "        \"\"\"\n",
        "\n",
        "        context = default(context, x) # if context is None, then do self-attention\n",
        "        output = None\n",
        "        ########################################################################\n",
        "        # TODO: Implement the forward pass of the `CrossAttention` class.\n",
        "        #       Hint: Remember there are multiple heads. rearrange and einsum maybe useful \n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "        return output\n",
        "\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads, d_head, dropout=0.1, context_dim=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn1 = None\n",
        "        self.attn2 = None\n",
        "        self.ff = None\n",
        "        self.norm1 = None\n",
        "        self.norm2 = None\n",
        "        self.norm3 = None\n",
        "        ########################################################################\n",
        "        # TODO: Define layers for the `BasicTransformerBlock` class.\n",
        "        #       Hint: 1. Use the CrossAttention and FeedForward Class\n",
        "        #             2. attn1 is self-attention; attn2 is cross-attention\n",
        "        #             3. We adopt LayerNorm as the normalization function here\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: input tensor [B, C, H, W] or [B, (H,W), C]\n",
        "          context: text embedding [B, K, E]\n",
        "        \"\"\"\n",
        "        output = None\n",
        "        ########################################################################\n",
        "        # TODO: Implement forward pass for the `BasicTransformerBlock` class.\n",
        "        #       Hint: 1. Can be done in three lines\n",
        "        #             2. We use residual connections in each layer\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "        return output\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block for image-like data.\n",
        "    First, project the input (aka embedding)\n",
        "    and reshape to b, t, d.\n",
        "    Then apply standard transformer action.\n",
        "    Finally, reshape to image\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, n_heads=4, d_head=32,\n",
        "                 depth=1, dropout=0.1, context_dim=512):\n",
        "        super().__init__()\n",
        "        input_dim = in_channels\n",
        "        inner_dim = n_heads * d_head\n",
        "        output_dim = in_channels\n",
        "        self.norm = nn.GroupNorm(num_groups=16, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "        self.proj_in = None\n",
        "        self.transformer_blocks = None\n",
        "        self.proj_out = None\n",
        "        ########################################################################\n",
        "        # TODO: Define layers for the `SpatialTransformer` class.\n",
        "        #       Hint: 1. proj_in and proj_out are projection layer for altering dimensions, with kernel_size=1 and stride=1\n",
        "        #             2. depth is the number of BasicTransformerBlock we use in our transformer blocks\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        \"\"\"\n",
        "        note: if no context is given, cross-attention defaults to self-attention\n",
        "        Args:\n",
        "            x: input image, shape: B, C, H, W\n",
        "            context: text embedding, shape: B, K, E\n",
        "        \"\"\"\n",
        "        output = None\n",
        "        ########################################################################\n",
        "        # TODO: Implement forward pass for the `SpatialTransformer` class.\n",
        "        #       Hint: 1. norm -> proj_in -> inner transformer -> proj_out\n",
        "        #             2. Again, there are multiple heads, rearrange maybe useful\n",
        "        #             2. We use residual connection with the output and the input\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CClV4D1_rbAh"
      },
      "outputs": [],
      "source": [
        "#@title Test for Attention\n",
        "in_channels = 16\n",
        "n_heads = 4\n",
        "d_head=8\n",
        "depth=1\n",
        "dropout=0\n",
        "context_dim=16\n",
        "model = SpatialTransformer(in_channels, n_heads, d_head,\n",
        "              depth, dropout, context_dim)\n",
        "\n",
        "# test\n",
        "model.load_state_dict(test_data['weights']['attention'])\n",
        "x, context = test_data[\"input\"][\"attention\"]\n",
        "y = test_data[\"output\"][\"attention\"]\n",
        "output = model(x, context).detach()\n",
        "check_error(\"attention\", output, y)\n",
        "\n",
        "# auto_grader\n",
        "model.load_state_dict(auto_grader_data[\"weights\"][\"attention\"])\n",
        "x, context = auto_grader_data[\"input\"][\"attention\"]\n",
        "output = model(x, context).detach()\n",
        "auto_grader_data[\"output\"][\"attention\"] = output "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "poTLcXaV7YNn"
      },
      "source": [
        "### Position Embeddings\n",
        "As the parameters of the neural network are shared across time (noise level), we employ sinusoidal position embeddings to encode t. This makes the neural network \"know\" at which particular time step (noise level) it is operating, for every image in a batch.\n",
        "\n",
        "The SinusoidalPositionEmbeddings module takes a tensor of shape `(batch_size, 1)` as input (i.e. the noise levels of several noisy images in a batch), and turns this into a tensor of shape `(batch_size, dim)`, with dim being the dimensionality of the position embeddings. This is then added to each residual block, as we have seen in the resnet blocks .\n",
        "\n",
        "We have already provide it to you, because it is not our focus in this assignment. But we recommend you to read the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HZfYqNpn7XYW"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPosEmb(nn.Module):\n",
        "    \"\"\"\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models.\n",
        "    Used for both position and time embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        # input: timesteps [B, ]\n",
        "        # output: embeddings [B, dim]\n",
        "        assert len(time.shape) == 1\n",
        "\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=device) * -emb)\n",
        "        emb = time.float()[:, None] * emb[None, :]\n",
        "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n",
        "        if self.dim % 2 == 1:  # zero pad\n",
        "            emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "        return emb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C6ZZGIxv9kg9"
      },
      "source": [
        "### Unet Model\n",
        "After implementing the different modules above, we will now utilize them to build our own actual diffusion model. The main components of the diffusion model is a Denoising UNet, which is also called the noisy predictor. \n",
        "\n",
        "Below is a picture of general Resnet-based Unet, which may help you understand the Unet architecture, but the actual architecture to be implemented will differ. To be more specific, we will use a mix of Resnet and Spatial Transformer blocks in the process of downsampling and upsampling.\n",
        "\n",
        "<img src=\"https://github.com/jun-tian/CS182_Project_diffusion/blob/main/images/resnet_unet.png?raw=true\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Am3stBG-LS7q"
      },
      "outputs": [],
      "source": [
        "#@title Build Your Unet Model\n",
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim = None,\n",
        "        out_dim = None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels = 3,\n",
        "        context_dim = None,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim) # init dimension after init_conv\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:])) # [(init_dim, dim), (dim, dim * 2), ...]\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        time_dim = dim * 4\n",
        "        resnet_block = partial(ResnetBlock, time_emb_dim=time_dim, dropout=dropout)\n",
        "        transformer_block = partial(SpatialTransformer, dropout=dropout, context_dim=context_dim)\n",
        "\n",
        "        ########################################################################\n",
        "        # TODO: Define layers for the `UNet` class.\n",
        "        ########################################################################\n",
        "\n",
        "        # Implement time embeddings (Hint: SinusoidalPosEmb->Linear->SiLU->Linear)\n",
        "        # We will change the dim in the first Linear layer(from dim to time_dim))\n",
        "        self.time_mlp = None\n",
        "\n",
        "        # Implement the initial convolutional layer (kernel size=7, padding=3)\n",
        "        self.init_conv = None\n",
        "\n",
        "        # Implement the downsampling process\n",
        "        # Hint: 1. use the in_out list to get the input and output dimension in each down sample iteration\n",
        "        #       2. We use Resnet -> Attention -> Resnet -> Attention -> Down Sample in each downsample iteration\n",
        "        #       3. The helper function Downsample maybe useful. \n",
        "        #       4. If it is the last layer before the middle layers, we do conv(kernel=3, padding=1)\n",
        "        #          instead of downsample\n",
        "        raise NotImplementedError()\n",
        "\n",
        "        # Implement the middle bottleneck layers\n",
        "        # Hint 1. Keep the dim as mid_dim\n",
        "        #      2. Resnet -> Attention -> Resnet\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "        # Implement the upsampling process\n",
        "        # Hint: 1. The structure is the same as downsampling \n",
        "        #         (Note that we use a residual connection here, so take care of the input dimension)\n",
        "        #       2. We use Resnet -> Attention -> Resnet -> Attention -> Upsample in each upsample iteration\n",
        "        #       3. The helper function Upsample maybe useful. \n",
        "        #       4. If it is the last iteration for upsampling, we do conv(kernel=3, padding=1) instead of up sample\n",
        "        raise NotImplementedError()\n",
        "\n",
        "        # Implement the final output layers\n",
        "        # Hint: 1. Since we have the residual connection in the Unet, \n",
        "        #          the input channel for the output layer is 2 x init_dim\n",
        "        #       2. First use a single layer to change the dimension from 2 x init_dim to\n",
        "        #          init_dim (kernel size=3, padding=1)\n",
        "        #       3. Then do normalization -> SiLU -> Conv(kernel size=3, padding=1)\n",
        "        self.final_conv = None\n",
        "        self.out = None\n",
        "        \n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x, time, context):\n",
        "        \"\"\"\n",
        "        forward pass of the unet model\n",
        "        Args:\n",
        "            x: input image [Batch_size, Channels, H, W]\n",
        "            time: timestep [Batch_size, 1]\n",
        "            text: text embedding [Batch_size, K, E]\n",
        "        Returns:\n",
        "            the predicted noise [Batch_size, Channels, H, W]\n",
        "        \"\"\"\n",
        "        output = None\n",
        "        ########################################################################\n",
        "        # TODO: Implement forward pass for the `UNet` class.\n",
        "        # Hint: 1. We use residual connections here, remember to save your temp parameters\n",
        "        #          in the downsampling phase and concat it to the parameters in upsampling\n",
        "        #       2. More specifically, save the model input and the outputs for each attention\n",
        "        #          layers in the downsampling\n",
        "        #       3. Concatenate them to the corresponding model output(before final_conv)\n",
        "        #          and inputs for each resnet block in the upsampling phase\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqRPLtFlrYNV"
      },
      "outputs": [],
      "source": [
        "#@title Test for Unet Model\n",
        "dim = 16\n",
        "init_dim=16\n",
        "model = Unet(dim, init_dim, \n",
        "              out_dim = None, dim_mults=(1, 2, 4),\n",
        "              channels = 1, context_dim = 10,\n",
        "              dropout=0)\n",
        "\n",
        "# test\n",
        "model.load_state_dict(test_data['weights']['unet'])\n",
        "x, timestep, context = test_data[\"input\"][\"unet\"]\n",
        "y = test_data[\"output\"][\"unet\"]\n",
        "output = model(x, timestep, context).detach()\n",
        "check_error(\"unet\", output, y)\n",
        "\n",
        "\n",
        "# auto_grader\n",
        "model.load_state_dict(auto_grader_data[\"weights\"][\"unet\"])\n",
        "x, timestep, context = auto_grader_data[\"input\"][\"unet\"]\n",
        "output = model(x, timestep, context).detach()\n",
        "auto_grader_data[\"output\"][\"unet\"] = output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IODTQROGJiU4"
      },
      "source": [
        "**Answer the folowing question in your writeup:**\n",
        "\n",
        "**Question**: Why might a U-Net by a good choice for the model backbone? List two reasons."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AnU2QOOmlkVe"
      },
      "source": [
        "## Diffusion Process\n",
        "After implementing different kinds of modules used in diffusion models, we now will walk through the algorithm itself. We will first talk about the basics of the diffusion process. Diffusion models are a class of **generative** models that break down the generation process into iterative denoising steps. They work by taking as input an image $x_0$, successively adding Gaussian noise to it, and then learning how to recover the original input by reversing this noising process. Once trained, a diffusion model can then generate new images by running this learned denoising process on randomly sampled noise.\n",
        "\n",
        "More concretely, given a starting input $x_0$ sampled from the data distribution $q(x)$, the diffusion forward process is iteratively adding Gaussian noise according to a variance schedule $\\beta_1, ... , \\beta_T$, producing $x_t$ with distribution $q(x_t|x_{t-1})$. The data is also scaled down by a factor of $\\sqrt{1-\\beta_t}$ at each step so so that the overall variance does not grow when adding noise.\n",
        "\n",
        "$$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\mu_t = \\sqrt{1-\\beta_t}x_{t-1}, \\Sigma_t=\\beta_t I)$$\n",
        "\n",
        "This corrresponds to a transition of the following Markov chain.\n",
        "\n",
        "![forwards](https://github.com/jun-tian/CS182_Project_diffusion/blob/main/images/forwards_diffusion.png?raw=true)\n",
        "\n",
        "Using the Markov chain property that each step only depends on the previous one, we can get the following expression for the posterior after repeatedly transitioning along the chain from timesepts 1 to T.\n",
        "\n",
        "$$q(x_{1:T}|x_{0}) = \\prod_{t=1}^{T} q(x_t|x_{t-1})$$\n",
        "\n",
        "However, this means that sampling $x_t$ at an arbitrary timestep would require sampling from the distribution $t$ times. The forward process can be made more efficient by reparameterizing. \n",
        "\n",
        "Defining $\\alpha_t = 1 - \\beta_t,\\; \\bar\\alpha_t = \\prod_{s=1}^{t} \\alpha_s, \\: \\epsilon \\sim N(0,I)$. It gives us the following closed form expresion:\n",
        "\n",
        "$$ \\begin{align}\n",
        "x_t & = \\sqrt{1-\\beta_t}\\,x_{t-1} + \\sqrt{\\beta_t}\\,\\epsilon \\\\\n",
        "&= \\sqrt{\\alpha_t}\\,x_{t-1} + \\sqrt{1-\\alpha_t}\\,\\epsilon \\\\\n",
        "&= \\;... \\\\\n",
        "&= \\sqrt{\\bar\\alpha_t}\\,x_{0} + \\sqrt{1-\\bar\\alpha_t}\\,\\epsilon \\\\\n",
        "\\end{align}$$\n",
        "\n",
        "These $\\alpha_t$ and $\\bar\\alpha_t$ can be precomputed for all timesteps at the start, so any $x_t$ can be sampled quickly. Given this noisy input $x_t$, the model is tasked with reconstructing $x_{t-1}$ by predicting the amount of noise in $x_t$ and then sampling from the posterior $q(x_{t-1}|x_t, x_0)$. We sample from $q(x_{t-1}|x_t, x_0)$ instead of $q(x_{t-1}|x_t)$ as without conditioning on $x_0$, the problem is intractable. $q(x_{t-1}|x_t, x_0)$ has variance $\\sigma_t = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_{t}} \\beta_t$\n",
        "\n",
        "This corresponds to learning the reverse process of the above Markov chain. We approximate the backwards distribution $q(x_{t-1}|x_t)$ as $p_{\\theta}(x_{t-1}|x_t)$ using our model.\n",
        "\n",
        "\n",
        "![backwards](https://github.com/jun-tian/CS182_Project_diffusion/blob/main/images/backwards_diffusion.png?raw=true)\n",
        "\n",
        "As $T \\longrightarrow \\infty$, the input signal is effectively destroyed, and can be assumed to be $\\mathcal{N}(0, I)$ noise. Thus, we can sample $x_T$ from $\\mathcal{N}(0, I)$ and treat it as our starting point. We can then run our diffusion model in an autoregressive style to generate a new image, feeding the denoised version of the image at one iteration to the input of the next iteration. This denosining process consists of T denoising steps, in which the model reconstructs $x_{T-1}$ from $x_T$, $x_{T-2}$ from $x_{T-1}$, and so on until it reaches $x_0$, a newly generated image.\n",
        "\n",
        "These algorithms for training and sampling a new image are summarized in the image below. For more details, read the original [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239.pdf) paper.\n",
        "\n",
        "![algorithms](https://github.com/jun-tian/CS182_Project_diffusion/blob/main/images/Algorithms.jpg?raw=true)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "36p-SZ9FoX7S"
      },
      "source": [
        "Diffusion models use a variance schedule, which specifies the variance, $\\beta_t$, of the Gaussian noise added to the input at timestep t. The LDM paper simply uses an increasing linear schedule, but other schedules, such as a cosine schedule described here [Nichol et al. 2021](https://arxiv.org/pdf/2102.09672.pdf), have given promising results. Below we have implemented four classic schedules for you. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UcNlnz1ALS7r"
      },
      "outputs": [],
      "source": [
        "#@title classic schedule functions\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
        "  \n",
        "schedules = {\n",
        "    \"cosine\": cosine_beta_schedule,\n",
        "    \"linear\": linear_beta_schedule,\n",
        "    \"quadratic\": quadratic_beta_schedule,\n",
        "    \"sigmoid\": sigmoid_beta_schedule\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "WW82zpg5Qsfh"
      },
      "outputs": [],
      "source": [
        "#@title build your scheduler\n",
        "class Scheduler:\n",
        "  \"\"\"Define the coefficients in the training and sampling algorithm\"\"\"\n",
        "  def __init__(self, schedule='linear', timesteps=200):\n",
        "    self.timesteps = timesteps\n",
        "\n",
        "    ########################################################################\n",
        "    # TODO: Define parameters for `Scheduler` class.\n",
        "    # Hint: 1. Feel free to use the schedules we have implemented for you to get betas\n",
        "    #       2. Refer back to the background section for how the variables are defined.\n",
        "    #       3. torch.cumprod and F.pad function maybe useful\n",
        "    ########################################################################\n",
        "    # define beta schedule\n",
        "    self.betas = None\n",
        "\n",
        "    # define alphas \n",
        "    alphas = None\n",
        "    alphas_cumprod = None\n",
        "    alphas_cumprod_prev = None\n",
        "    self.sqrt_reciprocal_alphas = None\n",
        "\n",
        "    # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "    self.sqrt_alphas_cumprod = None\n",
        "    self.sqrt_one_minus_alphas_cumprod = None\n",
        "\n",
        "    # calculations the variance for posterior q(x_{t-1} | x_t, x_0)\n",
        "    self.posterior_variance = None\n",
        "\n",
        "    ########################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLFZQ7QZrVrl"
      },
      "outputs": [],
      "source": [
        "#@title test for scheduler\n",
        "scheduler = Scheduler(schedule=\"linear\", timesteps=200)\n",
        "betas = scheduler.betas\n",
        "sqrt_reciprocal_alphas = scheduler.sqrt_reciprocal_alphas\n",
        "sqrt_alphas_cumprod = scheduler.sqrt_alphas_cumprod\n",
        "sqrt_one_minus_alphas_cumprod = scheduler.sqrt_one_minus_alphas_cumprod\n",
        "posterior_variance = scheduler.posterior_variance\n",
        "\n",
        "true_betas = test_data[\"output\"][\"betas\"]\n",
        "true_sqrt_reciprocal_alphas = test_data[\"output\"][\"sqrt_reciprocal_alphas\"]\n",
        "true_sqrt_alphas_cumprod = test_data[\"output\"][\"sqrt_alphas_cumprod\"]\n",
        "true_sqrt_one_minus_alphas_cumprod = test_data[\"output\"][\"sqrt_one_minus_alphas_cumprod\"]\n",
        "true_posterior_variance = test_data[\"output\"][\"posterior_variance\"]\n",
        "\n",
        "check_error(\"betas\", betas, true_betas)\n",
        "check_error(\"sqrt_reciprocal_alphas\", sqrt_reciprocal_alphas, true_sqrt_reciprocal_alphas)\n",
        "check_error(\"sqrt_alphas_cumprod\", sqrt_alphas_cumprod, true_sqrt_alphas_cumprod)\n",
        "check_error(\"sqrt_one_minus_alphas_cumprod\", sqrt_one_minus_alphas_cumprod, true_sqrt_one_minus_alphas_cumprod)\n",
        "check_error(\"posterior_variance\", posterior_variance, true_posterior_variance)\n",
        "\n",
        "scheduler = Scheduler(schedule=\"sigmoid\", timesteps=100)\n",
        "betas = scheduler.betas\n",
        "sqrt_reciprocal_alphas = scheduler.sqrt_reciprocal_alphas\n",
        "sqrt_alphas_cumprod = scheduler.sqrt_alphas_cumprod\n",
        "sqrt_one_minus_alphas_cumprod = scheduler.sqrt_one_minus_alphas_cumprod\n",
        "posterior_variance = scheduler.posterior_variance\n",
        "\n",
        "auto_grader_data[\"output\"][\"betas\"] = betas\n",
        "auto_grader_data[\"output\"][\"sqrt_reciprocal_alphas\"] = sqrt_reciprocal_alphas\n",
        "auto_grader_data[\"output\"][\"sqrt_alphas_cumprod\"] = sqrt_alphas_cumprod\n",
        "auto_grader_data[\"output\"][\"sqrt_one_minus_alphas_cumprod\"] = sqrt_one_minus_alphas_cumprod\n",
        "auto_grader_data[\"output\"][\"posterior_variance\"] = posterior_variance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WcnN_BYeQ9Dy"
      },
      "source": [
        "## Understand the Diffusion Process on a single image\n",
        "Below, you will observe the diffusion process on a single picture, which will help you understand the diffusion model. To be more specific, this is a process of gradually adding noise to the picture, done by sampling from $q(x_{1:T}|x_{0})$.\n",
        "\n",
        "Run the folllowing code to load an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HejnI7W3rT_x"
      },
      "outputs": [],
      "source": [
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuGLy3YHrSgs"
      },
      "outputs": [],
      "source": [
        "# Define the image transforms\n",
        "image_size = 128\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(), # turn into Numpy array of shape HWC, divide by 255\n",
        "    transforms.Lambda(lambda t: (t * 2) - 1),\n",
        "])\n",
        "\n",
        "reverse_transform = transforms.Compose([\n",
        "     transforms.Lambda(lambda t: (t + 1) / 2),\n",
        "     transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "     transforms.Lambda(lambda t: t * 255.),\n",
        "     transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "     transforms.ToPILImage(),\n",
        "])\n",
        "\n",
        "x_start = transform(image).unsqueeze(0)\n",
        "x_start.shape  # output torch.Size([1, 3, 128, 128])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMSxrM9BrRGI"
      },
      "outputs": [],
      "source": [
        "reverse_transform(x_start.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPEdjpe4L2PN"
      },
      "source": [
        "Implement the q sampling function below. This function will add noise to the input image. As we talked in the previous part, we need to use the coefficient from scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "X9K1gvrFLS7s"
      },
      "outputs": [],
      "source": [
        "#@title q sample\n",
        "linear_scheduler = Scheduler(schedule='linear', timesteps=200)\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "# forward diffusion (using the nice property)\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    x_noisy = None\n",
        "    ########################################################################\n",
        "    # TODO: Implement the forward diffusion process. (Sample from q(x_t | x_{0})\n",
        "    # Hint: 1. If noise is not defined, then you can sample it from a normal distribution\n",
        "    #       2. The extract function will help you to extract the t-th coefficients from scheduler(defined above as linear_scheduler)\n",
        "    ########################################################################\n",
        "    raise NotImplementedError()\n",
        "    ########################################################################\n",
        "    return x_noisy\n",
        "\n",
        "def get_noisy_image(x_start, t):\n",
        "    # add noise\n",
        "    x_noisy = q_sample(x_start, t=t)\n",
        "    \n",
        "    # turn back into PIL image\n",
        "    noisy_image = reverse_transform(x_noisy.squeeze())\n",
        "    return noisy_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx_EGuBzrO7c"
      },
      "outputs": [],
      "source": [
        "#@title test for q sample\n",
        "noise = test_data[\"input\"][\"q_sample\"]\n",
        "y = test_data[\"output\"][\"q_sample\"]\n",
        "t = torch.tensor([50])\n",
        "x_noisy = q_sample(x_start, t, noise)\n",
        "check_error(\"q_sample\", x_noisy, y)\n",
        "\n",
        "noise = auto_grader_data[\"input\"][\"q_sample\"]\n",
        "t = torch.tensor([100])\n",
        "x_noisy = q_sample(x_start, t, noise)\n",
        "auto_grader_data[\"output\"][\"q_sample\"] = x_noisy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnP0KtujLS7s"
      },
      "outputs": [],
      "source": [
        "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0]) + with_orig\n",
        "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        row = [image] + row if with_orig else row\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if with_orig:\n",
        "        axs[0, 0].set(title='Original image')\n",
        "        axs[0, 0].title.set_size(8)\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpIOaRYgrNOo"
      },
      "outputs": [],
      "source": [
        "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8boYPAvY-w_Y"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**Screenshot your visualization above** and include it in your submission of the written assignment. Also, describe the picture you observed briefly. What kind of process is this?\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "siBJGX3mURp2"
      },
      "source": [
        "## Train Your Diffusion Models\n",
        "To further understand how to train a diffusion model, we will let you train a small diffusion model based on the MNIST dataset below. Here we show a simple image which shows the overall process for training stage and inference stage. It is important to note that in the offical stable diffusion model, it adopts an AutoEncoderKL to encode the image from RGB or pixel space to latent space and downsample the image by a factor of $f$. This can reduce the demand on computing resources and speed up the inference time if the raw image size is large. \n",
        "\n",
        "However, in our demo, we won't use AutoEncoderKL, because our dataset MNIST is small enough, which means we will do the training and inference stages on the pixel level. But keep in mind, there is not much difference between pixel level and latent level when we input them into our model. The essential functionality of the model remains the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Eicr9hcDSm"
      },
      "source": [
        "<img src=\"https://github.com/jun-tian/CS182_Project_diffusion/blob/main/images/training.png?raw=true\" alt=\"resnt\" width=\"900\" height=\"500\" align=\"bottom\" />"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BPsTUOapepsC"
      },
      "source": [
        "Run the following code to prepare for the training stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wn80UBhvXzz0"
      },
      "outputs": [],
      "source": [
        "losses = {\n",
        "    'l1': F.l1_loss,\n",
        "    'l2': F.mse_loss,\n",
        "    'huber': F.smooth_l1_loss,\n",
        "}\n",
        "\n",
        "sample_text = [\n",
        "    'The number 0',\n",
        "    'The number 1',\n",
        "    'The number 2',\n",
        "    'The number 3',\n",
        "    'The number 4',\n",
        "    'The number 5',\n",
        "    'The number 6',\n",
        "    'The number 7',\n",
        "    'The number 8',\n",
        "    'The number 9'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6kJFXrSrKeU"
      },
      "outputs": [],
      "source": [
        "#@title Dataset\n",
        "dataset = load_dataset(\"mnist\", split=\"train\")\n",
        "\n",
        "image_size = 32\n",
        "channels = 1\n",
        "batch_size = 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1),\n",
        "])\n",
        "\n",
        "def data_transforms(examples):\n",
        "    examples[\"pixel_values\"] = [transform(image) for image in examples['image']]\n",
        "    examples[\"text\"] = [f\"The number {label}\" for label in examples['label']]\n",
        "\n",
        "    del examples['image']\n",
        "    del examples['label']\n",
        "    return examples\n",
        "\n",
        "transformed_dataset = dataset.with_transform(data_transforms)\n",
        "dataloader = DataLoader(transformed_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "batch = next(iter(dataloader))\n",
        "print(batch.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8y4-LyJrHsx"
      },
      "outputs": [],
      "source": [
        "#@title Initialize the Model\n",
        "model = Unet(\n",
        "    dim=image_size,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4,),\n",
        "    context_dim=512,\n",
        "    dropout=0.1\n",
        ")\n",
        "model.to(torch_device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "textembedder = FrozenCLIPEmbedder(max_length=5).to(torch_device)\n",
        "sample_embeddings = textembedder(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "2E5BSHTEaVfC"
      },
      "outputs": [],
      "source": [
        "#@title Define your diffusion class for sampleing and loss function\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, model, sample_emb=sample_embeddings, image_size=32, schedule=\"linear\", timesteps=200, loss_type=\"l1\"):\n",
        "      super().__init__()\n",
        "      self.model = model\n",
        "      self.image_size = image_size\n",
        "      self.scheduler = Scheduler(schedule, timesteps)\n",
        "      self.loss_fn = losses[loss_type]\n",
        "      self.sample_emb = sample_emb\n",
        "\n",
        "    def p_losses(self, x_start, t, context, noise=None):\n",
        "\n",
        "        loss = None\n",
        "        ########################################################################\n",
        "        # TODO: Define the loss function.\n",
        "        # Hint: 1. If the noise is not defined, then sample it from a normal distribution\n",
        "        #       2. Your previously implemented q_sample function will be useful to add noise\n",
        "        ########################################################################\n",
        "       \n",
        "        # 1. sample a noise\n",
        "        raise NotImplementedError()\n",
        "\n",
        "        # 2. add noise to the image\n",
        "        raise NotImplementedError()\n",
        "\n",
        "        # 3. predict the noise\n",
        "        raise NotImplementedError()\n",
        "\n",
        "        # 4. calculate the loss\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, x, t, t_index):\n",
        "\n",
        "        output = None\n",
        "        ########################################################################\n",
        "        # TODO: Define the p sampling function.\n",
        "        # Hint: 1. Recall the sampling algorithm we have shown you in the previous diffusion process part\n",
        "        #       2. The extract function can help you extract the t-th coefficient from the scheduler\n",
        "        #       3. We only need to add noise (z in the algorithm defined above) if t>1\n",
        "        ########################################################################\n",
        "        raise NotImplementedError()\n",
        "        ########################################################################\n",
        "        return output\n",
        "        \n",
        "    # (including returning all images)\n",
        "    @torch.no_grad()\n",
        "    def p_sample_loop(self, shape):\n",
        "        device = next(self.model.parameters()).device\n",
        "        b = shape[0]\n",
        "        # start from pure noise (for each example in the batch)\n",
        "        img = torch.randn(shape, device=device)\n",
        "        imgs = []\n",
        "\n",
        "        for i in tqdm(reversed(range(0, self.scheduler.timesteps)), desc='sampling loop time step', total=self.scheduler.timesteps):\n",
        "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
        "            imgs.append(img.cpu().numpy())\n",
        "        imgs = np.array(imgs)\n",
        "        return imgs\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, image_size, batch_size=10, channels=3):\n",
        "        return self.p_sample_loop(shape=(batch_size, channels, image_size, image_size))\n",
        "\n",
        "    def sample_t(self, batch_size):\n",
        "      return torch.randint(0, self.scheduler.timesteps, (batch_size,), device=torch_device).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "tmz7tzbsLS7t"
      },
      "outputs": [],
      "source": [
        "# define the save parameters\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "save_and_sample_every = 300\n",
        "\n",
        "# initialize the diffusion class for training\n",
        "timesteps = 200\n",
        "diffusion_model = Diffusion(model, schedule='linear', timesteps=timesteps, loss_type='huber')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDOYuNeprExC"
      },
      "outputs": [],
      "source": [
        "#@title Start Training\n",
        "epochs = 1 # need 14 minutes to train 1 epoch\n",
        "min_loss = 1000\n",
        "print(\"Start time:\", datetime.datetime.now())\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in (pbar := tqdm(enumerate(dataloader), desc='training', total=len(dataloader))):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_size = batch[\"pixel_values\"].shape[0]\n",
        "        text = batch[\"text\"]\n",
        "        batch = batch[\"pixel_values\"].to(torch_device)\n",
        "\n",
        "        # text embedding\n",
        "        context = textembedder(text)\n",
        "\n",
        "        # sample t uniformally for every example in the batch\n",
        "        t = diffusion_model.sample_t(batch_size)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = diffusion_model.p_losses(batch, t, context)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Show data\n",
        "        pbar.set_postfix(loss=loss.item(), epoch=f\"{epoch+1}/{epochs}\")\n",
        "\n",
        "        if loss < min_loss:\n",
        "            min_loss = loss.item()\n",
        "\n",
        "        # save generated images\n",
        "        if step != 0 and step % save_and_sample_every == 0:\n",
        "            milestone = step // save_and_sample_every\n",
        "            all_images = torch.Tensor(diffusion_model.sample(image_size, batch_size=10, channels=channels)).flatten(end_dim=1)\n",
        "            all_images = (all_images + 1) * 0.5\n",
        "            save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = len(sample_text))\n",
        "\n",
        "print(\"End time:\", datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AnxCoNmxKEg",
        "outputId": "2a33ed79-1a09-4251-ffce-43a0a1776d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The minimum loss 0.013211900368332863 is smaller than threshold loss 0.02\n"
          ]
        }
      ],
      "source": [
        "#@title Test your implementation\n",
        "auto_grader_data[\"output\"][\"min_loss\"] = min_loss\n",
        "check_loss(min_loss, threshold=0.02)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T3PtJ8NedBgD"
      },
      "source": [
        "To help you understand the q_sample process, we have sampled few images in the training stage. You can check them in the results file. Below, we will also show the performance of the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDRwKZCfrCYi"
      },
      "outputs": [],
      "source": [
        "# generate a sample image to show the model performance\n",
        "%matplotlib inline\n",
        "samples = torch.Tensor(diffusion_model.sample(image_size, batch_size=10, channels=channels))\n",
        "samples = (samples + 1) * 0.5\n",
        "random_index = 8\n",
        "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIoxG4jAq-VB"
      },
      "outputs": [],
      "source": [
        "# generate a dynamic gif to show the model performance (in the Files)\n",
        "random_index = 3\n",
        "\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for i in range(timesteps):\n",
        "    image = samples[i][random_index].permute(1, 2, 0).clamp(0, 1)\n",
        "    im = plt.imshow(image, cmap=\"gray\", animated=True)\n",
        "    ims.append([im])\n",
        "\n",
        "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
        "animate.save('diffusion.gif')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SuB0Vw-TdjbC"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**Screenshot one of your visualizations above** and include it in your submission of the written assignment. Answer the following question:\n",
        "- How does the model perform and does it meet your expectations? If not, what do you think are the directions for improvement?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHV6MtODspSQ"
      },
      "source": [
        "## Submission\n",
        "\n",
        "Download the file and upload it to the Gradescope.\n",
        "The Gradescope will run an autograder on the files you submit. \n",
        "\n",
        "It is very unlikely but still possible that your implementation might fail to pass some test cases on the Gradescope. Check your code carefully to ensure it correctness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoYHjIT7rAVg"
      },
      "outputs": [],
      "source": [
        "!rm submission.zip\n",
        "!zip submission.zip -r *.ipynb auto_grader_data.pt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
